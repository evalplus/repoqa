{"author":{"id":"MDQ6VXNlcjk2NTI5NQ==","is_bot":false,"login":"devfacet","name":"Fatih Cetinkaya"},"body":"@xenova : First of all thank you so much for your amazing work with this open source library. It opens up many possibilities.\r\n\r\nOne thing that caught my attention which is [FeatureExtractionPipeline](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.FeatureExtractionPipeline) can accept any amount of input regardless of the models' [sequence lengths](https://huggingface.co/spaces/mteb/leaderboard). Does it truncate or tokenize the data internally before applying it to the model? Is there documentation or an explanation about the implementation details?","comments":[{"id":"IC_kwDOI9T9VM5tGV2d","author":{"login":"xenova"},"authorAssociation":"OWNER","body":"Hi there üëã Thanks so much for your kind words! ü§ó \r\n\r\nYes, it does perform truncation of the (tokenized) input prior to model execution (see the parent class' `_call` function):\r\nhttps://github.com/xenova/transformers.js/blob/768a2e26d7f34746caa2b102f55dbd270c5d6f36/src/pipelines.js#L126-L130\r\n\r\nThe amount it truncates is determined by the tokenizer's `max_model_length` (which can be found in the tokenizer_config.json, e.g., [here](https://huggingface.co/Xenova/bert-base-cased/blob/main/tokenizer_config.json#L6))","createdAt":"2023-11-28T17:47:11Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/xenova/transformers.js/issues/421#issuecomment-1830378909","viewerDidAuthor":false},{"id":"IC_kwDOI9T9VM5tG5AP","author":{"login":"devfacet"},"authorAssociation":"NONE","body":"Is there a way to set `truncation` to `false` and return error if the given text is larger than `model_max_length`? Or do I need to implement that logic for myself before passing text to the `extractor` function?","createdAt":"2023-11-28T19:18:51Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/xenova/transformers.js/issues/421#issuecomment-1830522895","viewerDidAuthor":false},{"id":"IC_kwDOI9T9VM5tHQOp","author":{"login":"xenova"},"authorAssociation":"OWNER","body":"Sure! You can decompose the pipeline into it's separate parts: (1) Tokenization, followed by (2) Inference. Here's some example code:\r\n\r\n[Test in jsFiddle](https://jsfiddle.net/bksa5pgr/)\r\n\r\n```js\r\nimport { env, AutoModel, AutoTokenizer } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.9.0';\r\nenv.allowLocalModels=false; // disable local model check\r\n\r\nconst model_id = 'Xenova/all-MiniLM-L6-v2';\r\n\r\nconst tokenizer = await AutoTokenizer.from_pretrained(model_id);\r\nconst model = await AutoModel.from_pretrained(model_id);\r\n\r\n// Example function that generates token embeddings for text,\r\n// but throws an error if the text is too long\r\nasync function generateTokenEmbeddings(text){\r\n  // (1) Tokenization\r\n  const model_inputs = tokenizer(text);\r\n  const numberOfTokens = model_inputs.input_ids.dims.at(-1);\r\n\r\n  // Check that inputs are valid\r\n  if(numberOfTokens > tokenizer.model_max_length){\r\n    throw new Error(`Input is larger than max model length (${numberOfTokens} > ${tokenizer.model_max_length})`);\r\n  }\r\n\r\n  // Input is valid\r\n  console.log(`'${text.slice(0,20)}...' is valid (${numberOfTokens} tokens).`)\r\n\t\r\n  // (2) Run model\r\n  const output = await model(model_inputs);\r\n  console.log(output);\r\n}\r\n\r\nconst text = \"Hello world!\"\r\nawait generateTokenEmbeddings(text) // Works\r\n// {\r\n//  last_hidden_state: {\r\n//    data: [object Float32Array],\r\n//    dims: [1, 5, 384],\r\n//    size: 1920,\r\n//    type: \"float32\"\r\n//  }\r\n//}\r\n\r\nconst text2 = \"This won't work \".repeat(200)\r\nawait generateTokenEmbeddings(text2) // this throws an error\r\n// Error: \"Input is larger than max model length (1002 > 512)\"\r\n\r\n```\r\n\r\n\r\n\r\n(see [tokenizers](https://huggingface.co/docs/transformers.js/api/tokenizers) and [models](https://huggingface.co/docs/transformers.js/api/models) docs for more information).","createdAt":"2023-11-28T19:50:48Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/xenova/transformers.js/issues/421#issuecomment-1830618025","viewerDidAuthor":false},{"id":"IC_kwDOI9T9VM5tHSRz","author":{"login":"devfacet"},"authorAssociation":"NONE","body":"Great! Thank you üôè ","createdAt":"2023-11-28T19:56:55Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/xenova/transformers.js/issues/421#issuecomment-1830626419","viewerDidAuthor":false},{"id":"IC_kwDOI9T9VM5tflCj","author":{"login":"devfacet"},"authorAssociation":"NONE","body":"@xenova : Quick question: Some models have a typo (e.g., https://huggingface.co/Xenova/e5-small-v2/blob/a59d88d9e737bbaf6becc14ed014a9a7c82067e4/tokenizer_config.json#L7) which results in invalid `model_max_length` values. Should I track them myself, or is there another way to calculate `model_max_length` values?","createdAt":"2023-12-02T02:09:19Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/xenova/transformers.js/issues/421#issuecomment-1836994723","viewerDidAuthor":false},{"id":"IC_kwDOI9T9VM5tgEqx","author":{"login":"xenova"},"authorAssociation":"OWNER","body":"Oh yes good point. In that case, you can use `model.config.max_position_embeddings` (in fact, this is probably the better option). To be safe, you can take the minimum of the two and use that instead.","createdAt":"2023-12-02T11:20:50Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[{"content":"THUMBS_UP","users":{"totalCount":1}}],"url":"https://github.com/xenova/transformers.js/issues/421#issuecomment-1837124273","viewerDidAuthor":false}],"createdAt":"2023-11-28T17:28:28Z","labels":[{"id":"LA_kwDOI9T9VM8AAAABMvEY-w","name":"question","description":"Further information is requested","color":"d876e3"}],"title":"[Question] FeatureExtractionPipeline input length","updatedAt":"2023-12-02T11:20:52Z"}
